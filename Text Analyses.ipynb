{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis - Florent Andre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this analysis, I will the analyse the customer's sentiments from reviews by Amazon users left up to October 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\past\\types\\oldstr.py:33: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\d\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nose\\config.py:178: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\decorators.py:68: DeprecationWarning:\n",
      "\n",
      "`formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\featstruct.py:1296: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\d\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\featstruct.py:2092: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\d\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\sem\\evaluate.py:311: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\ \n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\sem\\relextract.py:135: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\w\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\sem\\relextract.py:414: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\sem\\boxer.py:779: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\d\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\sem\\drt.py:720: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\ \n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\sem\\drt.py:721: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\ \n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\grammar.py:1188: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\*\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\grammar.py:1360: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\w\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\text.py:583: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\w\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tree.py:99: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\ \n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tree.py:646: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tree.py:652: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tree.py:654: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tree.py:656: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tree.py:883: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\$\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\parse\\chart.py:1038: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\*\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\parse\\chart.py:1077: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\*\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\parse\\chart.py:1132: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\*\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\parse\\chart.py:1152: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\*\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\parse\\chart.py:1222: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\*\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\parse\\chart.py:1245: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\*\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\parse\\featurechart.py:273: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\*\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\parse\\featurechart.py:372: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\*\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tag\\sequential.py:740: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\w\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tag\\sequential.py:734: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\W\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tag\\sequential.py:732: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\classify\\rte_classify.py:62: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\w\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1456: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\regexp.py:103: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\w\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\regexp.py:196: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\w\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\repp.py:135: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\(\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\texttiling.py:96: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\-\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\texttiling.py:229: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\w\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\toktok.py:53: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\]\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\toktok.py:55: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\[\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\toktok.py:62: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\|\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\treebank.py:269: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\]\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\treebank.py:273: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\treebank.py:277: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py:1355: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\ \n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\util.py:373: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\S\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\util.py:519: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\w\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\util.py:528: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py:73: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\{\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py:218: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\{\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py:431: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\g\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py:477: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\g\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py:517: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\{\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py:518: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\g\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py:583: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\g\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py:718: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\{\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py:724: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\g\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py:789: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\}\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py:795: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\g\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py:908: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\{\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py:908: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\{\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py:1189: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\inference\\discourse.py:44: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\ \n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\stem\\lancaster.py:195: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\*\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\stem\\lancaster.py:228: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\*\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\stem\\porter.py:181: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\m\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py:116: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py:123: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py:126: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py:128: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py:311: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py:335: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py:364: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py:374: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py:383: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py:392: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py:401: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py:62: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\util.py:635: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\d\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\util.py:859: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:80: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\timit.py:168: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\bracket_parse.py:214: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\xmldocs.py:233: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\toolbox.py:210: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\_\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\bnc.py:29: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\w\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\switchboard.py:116: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\w\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\childes.py:282: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\d\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\framenet.py:2761: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\w\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\udhr.py:31: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\-\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\twitter.py:54: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\draw\\cfg.py:166: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\draw\\cfg.py:166: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\draw\\cfg.py:171: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\ccg\\combinator.py:232: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\Y\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\treetransforms.py:108: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\ \n",
      "\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\C3134161\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\C3134161\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\C3134161\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\boto\\__init__.py:1142: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\c\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\boto\\pyami\\config.py:98: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning:\n",
      "\n",
      "`scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing all libraries \n",
    "# For data and matrix manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# For visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import cufflinks as cf\n",
    "import plotly.graph_objs as go\n",
    "from chart_studio.plotly import iplot\n",
    "import pyLDAvis.gensim  # Visualise topics\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from collections import Counter\n",
    "# For string manipulation\n",
    "import re \n",
    "import string\n",
    "# For text pre-processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag  # Tag words with parts of speech\n",
    "from collections import defaultdict  # Dictionaries that have a backup value\n",
    "# Necessary dependencies from NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from textblob import TextBlob # For assigning sentiment polarity scores\n",
    "from sklearn.feature_extraction.text import CountVectorizer # For extracting features such as the document-term matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB # Naive Bayes model\n",
    "from sklearn.tree import DecisionTreeClassifier # Decision Tree model\n",
    "from sklearn.metrics import accuracy_score, classification_report # To evaluate the models \n",
    "from gensim.models.ldamodel import LdaModel  # Topic extraction\n",
    "from gensim.models.phrases import Phrases  # ngrams\n",
    "from gensim import corpora  # Vectorization\n",
    "\n",
    "# Ignore pink warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#from PIL import Image\n",
    "#from tqdm import tqdm\n",
    "#import os\n",
    "#import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId ProfileName  HelpfulnessNumerator  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW  delmartian                     1   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time                Summary  \\\n",
       "0                       1      5  1303862400  Good Quality Dog Food   \n",
       "\n",
       "                                                Text  \n",
       "0  I have bought several of the Vitality canned d...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Reviews.csv\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is of interest\n",
    "Let's get an overview of the Score Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    363122\n",
       "4     80655\n",
       "1     52268\n",
       "3     42640\n",
       "2     29769\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of each score label in %\n",
    "This allows us to have a clear overall understanding on the performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "review = [52268,29769,42640,80655,363122]\n",
    "\n",
    "my_labels = [1, 2, 3, 4, 5]\n",
    "plt.pie(review,labels=my_labels,autopct='%1.1f%%')\n",
    "plt.title('% Of scoring label')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "\n",
    "I will capture the \"text\" column only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"Text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleansing for Punctuation\n",
    "\n",
    "I will carry out the cleansing of text through multiple steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I got two Teas and one type of iced coffe and the rest were HOLIDAY COFFEES and APPLE CIDER!  Spicy Eggnog? Gingerbread? Bait and swtich.  The only redeeming factor - Amazon was great and issued me a refund.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull a random text to check potential problems.\n",
    "df[\"Text\"].iloc[2082]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert everything to lower case for simplicity\n",
    "df[\"Text\"] = df[\"Text\"].str.lower()\n",
    "# Remove @, & and #, \n",
    "df[\"Text\"] = df[\"Text\"].replace(to_replace=r\"[#@&]\\w\\s\\+\", value=\" \", regex=True)\n",
    "# Deal with quote marks being difficult\n",
    "df[\"Text\"] = df[\"Text\"].replace(to_replace=\"(\\u2019)\", value=\"'\", regex=True)\n",
    "# Replace the most common contractions\n",
    "df[\"Text\"] = df[\"Text\"].replace(to_replace=\"n't\", value=\" not\", regex=True)\n",
    "df[\"Text\"] = df[\"Text\"].replace(to_replace=\"it's\", value=\"it is\", regex=True)\n",
    "df[\"Text\"] = df[\"Text\"].replace(to_replace=\"i've\", value=\"i have\", regex=True)\n",
    "df[\"Text\"] = df[\"Text\"].replace(to_replace=\"i'm\", value=\"i am\", regex=True)\n",
    "df[\"Text\"] = df[\"Text\"].replace(to_replace=\"'s\", value=\"\", regex=True)\n",
    "# Remove ampersands\n",
    "df[\"Text\"] = df[\"Text\"].replace(to_replace=r\"&amp\", value=\"\", regex=True)\n",
    "# Remove links\n",
    "df[\"Text\"] = df[\"Text\"].replace(to_replace=r\"http[s]?:\\/\\/[^\\s]+\", value=\"\", regex=True)\n",
    "# Remove numbers\n",
    "df[\"Text\"] = df[\"Text\"].replace(to_replace=r\"\\d+\", value=\"\", regex=True)\n",
    "# Remove standard punctuation and replace with spaces\n",
    "df[\"Text\"] = df[\"Text\"].replace(to_replace=r\"\\W\", value=\" \", regex=True)\n",
    "# Remove extra spaces\n",
    "df[\"Text\"] = df[\"Text\"].replace(to_replace=r\"\\s+\", value=\" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i got two teas and one type of iced coffe and the rest were holiday coffees and apple cider spicy eggnog gingerbread bait and swtich the only redeeming factor amazon was great and issued me a refund '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Text\"].iloc[2082]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleansing for Words\n",
    "\n",
    "I will Tokenize the column Text so whatever is unstructured will become more usable at a later stage when I will use my machine learning models.\n",
    "\n",
    "I will use lemmatisation to reduce the words back to their roots, such as \"went\" and \"going\" to \"Go\"\n",
    "\n",
    "I will also use stopwords which remove all words that don't add any value to my analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise the words so they can be dealt with individually\n",
    "df[\"Text\"] = df[\"Text\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating of an Ad-Hoc list of specific words wanted removed.\n",
    "newStopWords = ['amazon']\n",
    "# Main common stopwords list.\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "# Adding Ad-hoc list to the main list\n",
    "stopwords.extend(newStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the words by removing the words from the stopwords list and words with <=3 letters\n",
    "df[\"Text\"] = df[\"Text\"].apply(lambda x: [y for y in x if y not in stopwords and len(y) > 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-df2b88eec972>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Perform the lemmatization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_lemma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3589\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3590\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3591\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3593\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-df2b88eec972>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Perform the lemmatization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_lemma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-df2b88eec972>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Perform the lemmatization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_lemma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-df2b88eec972>\u001b[0m in \u001b[0;36mget_lemma\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# function to lemmatize a sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_lemma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtag_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \"\"\"\n\u001b[1;32m--> 161\u001b[1;33m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             AP_MODEL_LOC = 'file:' + str(\n\u001b[1;32m--> 144\u001b[1;33m                 \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'taggers/averaged_perceptron_tagger/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mPICKLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m             )\n\u001b[0;32m    146\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;31m# Is the path item a zipfile?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mpath_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    641\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mZipFilePathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Object to lemmatize words \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Dictionary to convert between part-of-speech tags and the ones the lemmatizer understands\n",
    "tag_dict = defaultdict(lambda : wordnet.NOUN)\n",
    "tag_dict['J'] = wordnet.ADJ\n",
    "tag_dict['V'] = wordnet.VERB\n",
    "tag_dict['R'] = wordnet.ADV\n",
    "\n",
    "# function to lemmatize a sentence\n",
    "def get_lemma(word):\n",
    "    tag = pos_tag([word])[0][1][0]\n",
    "    tag = tag_dict[tag]\n",
    "    \n",
    "    return lemma.lemmatize(word, tag)\n",
    "\n",
    "# Perform the lemmatization\n",
    "df[\"Text\"] = df[\"Text\"].apply(lambda x: [get_lemma(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-358850ba0207>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Perform the lemmatization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlemmed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3589\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3590\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3591\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3593\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-358850ba0207>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Perform the lemmatization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlemmed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-358850ba0207>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Perform the lemmatization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlemmed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def lemmed(text, cores=6): # tweak cores as needed\n",
    "    with Pool(processes=cores) as pool:\n",
    "        wnl = WordNetLemmatizer()\n",
    "        result = pool.map(wnl.lemmatize, text)\n",
    "    return result\n",
    "\n",
    "# Perform the lemmatization\n",
    "df[\"Text\"] = df[\"Text\"].apply(lambda x: [lemmed(text, cores=6) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the lists of words back into strings\n",
    "df[\"Text\"] = df[\"Text\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Text\"].iloc[2082]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "Possible exploration of specific word within the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = input(\"word: \")\n",
    "df[df[\"Text\"].str.contains(search_term)][\"Text\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "Creating multiple models to visualise and analyse various aspect of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Word Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object that converts strings into a sparse matrix\n",
    "vectorizer = CountVectorizer(max_features= 10000 , min_df=0.01, ngram_range= (1,1))\n",
    "# Convert the text column to a sparse matrix\n",
    "df_dtm = vectorizer.fit_transform(df[\"Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of a dataframe with terms and their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the frequency of each term\n",
    "term_frequencies = df_dtm.sum(axis=0)\n",
    "# Match frequencies to terms\n",
    "frequent_terms = [(word, term_frequencies[0, id]) for word, id in vectorizer.vocabulary_.items()]\n",
    "# Convert the word-frequency pairings to a dataframe\n",
    "term_df = pd.DataFrame(columns=[\"term\", \"frequency\"], data=frequent_terms)\n",
    "# Sort the dataframe\n",
    "term_df.sort_values(by=\"frequency\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Word Visualisation - Barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(term_df.head(20), x=\"frequency\", y=\"term\", title='The 20 most Commmon terms', orientation='h', \n",
    "             width=700, height=700, color=\"term\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation - Wordcloud of the most frequent terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of the most frequent terms (required for word clouds)\n",
    "term_dict = term_df.set_index('term')['frequency'].to_dict()\n",
    "# Create a wordcloud of most frequent terms\n",
    "wc = WordCloud(width=800, height=300, background_color=\"black\", max_words=75)\n",
    "wc.generate_from_frequencies(term_dict)\n",
    "# Display the cloud\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(wc, interpolation=\"kaiser\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of the most 30 positive and most negative terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign sentiment to each term\n",
    "term_df[\"sentiment\"] = term_df[\"term\"].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe capturing the 30 worst feedback from customers\n",
    "top_30_negative = term_df.sort_values(by=\"sentiment\", ascending=True)[[\"term\", \"frequency\", \"sentiment\"]].head(30)\n",
    "# plot\n",
    "fig = px.treemap(top_30_negative, path=['term'], values='frequency',title='Most Common Negative Terms')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe capturing the 30 best feedback from customers\n",
    "top_30_positive = term_df.sort_values(by=\"sentiment\", ascending=False)[[\"term\", \"frequency\", \"sentiment\"]].head(30)\n",
    "# plot\n",
    "fig = px.treemap(top_30_positive, path=['term'], values='frequency',title='Most Common Positive Terms')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of most frequent terms ((required for word clouds)\n",
    "top_30_negative_dict = top_30_negative.set_index('term')['sentiment'].to_dict()\n",
    "# Create a wordcloud of most frequent terms\n",
    "wc_N = WordCloud(width=800, height=300, background_color=\"white\",max_words=75)\n",
    "wc_N.generate_from_frequencies(top_30_negative_dict)\n",
    "# Display the cloud\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(wc_N, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of most frequent terms ((required for word clouds)\n",
    "top_30_positive_dict = top_30_positive.set_index('term')['sentiment'].to_dict()\n",
    "# Create a wordcloud of most frequent terms\n",
    "wc_P = WordCloud(width=800, height=300, background_color=\"white\",max_words=75)\n",
    "wc_P.generate_from_frequencies(top_30_positive_dict)\n",
    "# Display the cloud\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(wc_P, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis model\n",
    "\n",
    "Investigation of the sentiments of the comment in \"Text\", using the library textblob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign sentiment to each comment\n",
    "df[\"sentiment\"] = df[\"Text\"].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print visualisations from cufflinks library offline\n",
    "cf.go_offline()\n",
    "\n",
    "# Plot an interactive distribution plot of sentiment scores in plotly\n",
    "df['sentiment'].iplot(\n",
    "    kind='hist',\n",
    "    bins=50,\n",
    "    xTitle='polarity',\n",
    "    theme = 'pearl',\n",
    "    colorscale = 'plotly',\n",
    "    linecolor='black',\n",
    "    yTitle='count',\n",
    "    title='Sentiment Polarity Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification model\n",
    "\n",
    "Building a machine learning model to predict the sentiment of the feedback from the customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column to categorises sentiment\n",
    "df['sentiment_category'] = np.where(df['sentiment'] > 0.125, 'Positive', \n",
    "                                          np.where(df['sentiment'] < -0.025, \"Negative\", 'Neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an Histogram to view the frequency of each sentiments\n",
    "fig = px.histogram(df, x=\"sentiment_category\",color=\"sentiment_category\")\n",
    "fig.update_layout(\n",
    "    title_text='Sentiment of Comments', # title of plot\n",
    "    xaxis_title_text='Sentiment', # xaxis label\n",
    "    yaxis_title_text='Count', # yaxis label\n",
    "    bargap=0.2, \n",
    "    bargroupgap=0.1\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'sentiments' column as it is now redundant to use.\n",
    "df.drop('sentiment', axis=1, inplace=  True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the bag-of-words model.\n",
    "vector = CountVectorizer(max_features= 10000 , min_df=0.01, ngram_range= (1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split function\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['Text'], df['sentiment_category'],  \n",
    "                                                   test_size = 0.3, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the split\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the bag of words model to the entire text using the fit function\n",
    "vector.fit(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature transformation to both x_train, and x_test\n",
    "x_train_bow = vector.transform(x_train)\n",
    "x_test_bow = vector.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of features \n",
    "print(x_train_bow.shape, x_test_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning algorithms\n",
    "\n",
    "I will compare a Naive Bayes model and a Decision Tree model and make a decision on the final model to use based on the accuracy result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Multinomial Naive Bayes classifier\n",
    "nb = MultinomialNB()\n",
    "# Train the model using the training sets\n",
    "nb.fit(x_train_bow, y_train)\n",
    "# Predict Output\n",
    "predictions = nb.predict(x_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the accuracy score\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "tree = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "tree = tree.fit(x_train_bow, y_train)\n",
    "# Predict the response for test dataset\n",
    "predictions_tree = tree.predict(x_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predictions_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report\n",
    "\n",
    "Printing the classification report based on the Decision Tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "print(classification_report(y_test, predictions_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion Text Classification.\n",
    "\n",
    "My prediction data label correctly Positive sentiments with 91% accuracy, label Negative sentiments correctly with an accuracy of 66% which needs improvement, and label Neutral sentiment correctly with an accuracy of 63%.\n",
    "\n",
    "Through recall, I can conclude that 90% positive sentiments were classified correctly, and 62% for the negative sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract just the Text column as a list of lists\n",
    "df_topics = list(df[\"Text\"].apply(lambda x: x.split(\" \")))\n",
    "# Create a gensim dictionary of terms from the Text and map each word to an id\n",
    "topic_dict = corpora.Dictionary(df_topics)\n",
    "# Use the dictionary to generate a corpus from Text and convert the keywords into dictionary references\n",
    "topic_corpus = [topic_dict.doc2bow(text) for text in df_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will identify 5 topics, at random\n",
    "topic_model = LdaModel(topic_corpus, num_topics=5, id2word=topic_dict, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the visualisation display \n",
    "lda_display = pyLDAvis.gensim.prepare(topic_model, topic_corpus, topic_dict, sort_topics=True)\n",
    "# Display the visualisation\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion of the Topic Analysis\n",
    "\n",
    "The above LDA display allows us to see the 30 most common terms per Topic, distributed, for this exercise in 5 different Topics, while indicating the most important Topic, represented by the bubble's sizes.\n",
    "\n",
    "Within each topic, I can see the frequency of a term overall and the frequency of that same term within a specific topic. I can also visualise in which Topic a term appears, and its frequency represented by the bubble's sizes.\n",
    "\n",
    "In term of importance of each topic over the entire corpus. Topic 1 is the most important Topic capturing 29.6% of the terms, followed by 2 with 24.1%, then 3 with 19.8%. Topic 4 captures 17.1% of the terms and, lastly, 5 captures 9.4%.\n",
    "\n",
    "Based on the words captured in each Topics, Topic 1 and 2 are capturing most of the positive terms. The remaining bubbles captures more neutral words which I could add to the stop_words and run the analyse again to check the new result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
